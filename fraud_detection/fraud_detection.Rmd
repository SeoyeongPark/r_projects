---
title: "fraud_detection"
author: "Seoyeong Park"
output: 
  pdf_document: default
  HTML_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Credit card fraud detection with machine learning in R

### importing libraries and dataset
```{r}
#install.packages("ranger")
#install.packages("caret")
```

```{r}
library(ranger)
library(caret)
library(data.table)
creditcard_data <- read.csv("creditcard.csv")
```

### Data exploration
Explore data frame with head(), tail()
```{r}
dim(creditcard_data)
head(creditcard_data, 10)
tail(creditcard_data, 5)
```

```{r}
table(creditcard_data$Class)
summary(creditcard_data$Amount)
names(creditcard_data)
var(creditcard_data$Amount)
sd(creditcard_data$Amount)
```

### Data manipulation
Standardize dataset
```{r}
creditcard_data$Amount = scale(creditcard_data$Amount)
NewData = creditcard_data[,-c(1)]
head(NewData)
```

### Data Modeling
Split dataset to training set and test set with a ratio of 0.8.   
Eventually, 80% of data will be attributed to train_data, and 20% of data will be attributed to test data. Then, I will find dimensions using dim().
```{r}
library(caTools)
set.seed(123)
data_sample = sample.split(NewData$Class, SplitRatio=0.8)
train_data = subset(NewData, data_sample==TRUE)
test_data = subset(NewData, data_sample==FALSE)
dim(train_data)
dim(test_data)
```

### Fitting logistic regression model
To fit the first model, I wil try with logistic regression. 
A logistic regression is used for modeling the outcome probability of a class, fraud/genuine. I will summarize data and visualize it.
```{r}
Logistic_Model = glm(Class~., test_data, family=binomial())
summary(Logistic_Model)
plot(Logistic_Model)
```

To assess the performance of the model, I will delineate the ROC curve(Receiver optimistic characteristics). 
```{r}
library(pROC)
lr.predict <- predict(Logistic_Model, test_data, probability = TRUE)
auc.gbm = roc(test_data$Class, lr.predict, plot = TRUE, col = "blue")
```

### Fitting a decision tree model
```{r}
library(rpart)
library(rpart.plot)
decisionTree_model <- rpart(Class ~ . , creditcard_data, method = 'class')
predicted_val <- predict(decisionTree_model, creditcard_data, type='class')
probability <- predict(decisionTree_model, creditcard_data, type = 'prob')
rpart.plot(decisionTree_model)
```

### Artificial neural network
Artificial neural network allows to learn the patterns using the data and perform classification. In this case, I set a threshold as 0.5 so that values above 0.5 will correspond to 1 and the rest will be 0. 
```{r}
library(neuralnet)
ANN_model = neuralnet(Class~., train_data, linear.output=FALSE)
plot(ANN_model)

predANN=compute(ANN_model, test_data)
resultANN=predANN$net.result
resultANN=ifelse(resultANN>0.5, 1, 0)
```

### Gradient Boosting
This algorithm is used to perform classification and regression tasks. This model comprises of several underlying ensemble models like weak decision trees. These combine together to form a strong model of gradient boosting. 
```{r}
library(gbm, quietly=TRUE)
system.time(model_gbm <- gbm(Class~.,
                             distribution="bernoulli",
                             data=rbind(train_data, test_data),
                             n.trees=500,
                             interaction.depth=3,
                             n.minobsinnode=100,
                             shrinkage=0.01,
                             bag.fraction=0.5,
                             train.fraction=nrow(train_data)/(nrow(train_data)+nrow(test_data))))
gbm.iter=gbm.perf(model_gbm, method="test")

model.influence=relative.influence(model_gbm, n.trees=gbm.iter, sort. = TRUE)
plot(model_gbm)

gbm_test=predict(model_gbm, newdata=test_data, n.trees=gbm.iter)
gbm_auc=roc(test_data$Class, gbm_test, plot=TRUE, col="red")
print(gbm_auc)
```

